#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
TranscriberUltra - Module d'Optimisation Ultra-Rapide pour WhatsApp Extractor V2
Version 1.0.0 - Optimis√© pour 2 cl√©s API
Compatible avec l'interface existante - Drop-in replacement
"""

import os
import json
import time
import logging
import asyncio
import aiohttp
import openai
from typing import List, Dict, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import threading
import queue
import subprocess
from collections import OrderedDict
from utils import normalize_contact_name_for_filesystem
from concurrent.futures import ThreadPoolExecutor, as_completed

# Import OpenAI
try:
    from openai import OpenAI
except ImportError:
    print("ERREUR: OpenAI non install√©. Ex√©cutez: pip install openai")
    sys.exit(1)

# Configuration du logger
logger = logging.getLogger('whatsapp_extractor')  # Utiliser le m√™me logger que le syst√®me principal

# =============================================================================
# UTILITAIRES DE NORMALISATION DES NOMS
# =============================================================================

# =============================================================================
# COMPRESSION AUDIO POUR OPTIMISATION WHISPER API
# =============================================================================

def get_ffmpeg_path() -> str:
    """Trouve le chemin FFmpeg disponible"""
    ffmpeg_paths = [
        'ffmpeg',  # PATH syst√®me
        os.path.join(os.path.dirname(os.path.dirname(__file__)), 'ffmpeg', 'bin', 'ffmpeg.exe'),
        os.path.join(os.getcwd(), 'ffmpeg', 'bin', 'ffmpeg.exe'),
    ]

    for ffmpeg_path in ffmpeg_paths:
        try:
            result = subprocess.run([ffmpeg_path, '-version'],
                                  capture_output=True,
                                  timeout=5)
            if result.returncode == 0:
                return ffmpeg_path
        except:
            continue

    return None


def compress_audio_for_whisper(input_path: str, output_path: str = None) -> Tuple[str, bool]:
    """
    Compresse un fichier audio pour optimiser l'envoi √† Whisper API.

    Param√®tres optimaux bas√©s sur la recherche:
    - Bitrate: 16 kbps (suffisant pour speech, r√©duit ~4-5x la taille)
    - Sample rate: 16 kHz (Whisper downsample √† 16k internement)
    - Channels: Mono (la voix n'a pas besoin de st√©r√©o)

    Args:
        input_path: Chemin du fichier audio original
        output_path: Chemin de sortie (optionnel, g√©n√®re automatiquement)

    Returns:
        Tuple (chemin_fichier_compress√©, succ√®s)
    """
    if not os.path.exists(input_path):
        logger.error(f"‚ùå Fichier source introuvable: {input_path}")
        return input_path, False

    # G√©n√©rer le chemin de sortie si non sp√©cifi√©
    if output_path is None:
        base, ext = os.path.splitext(input_path)
        output_path = f"{base}_compressed.mp3"

    # Trouver FFmpeg
    ffmpeg_path = get_ffmpeg_path()
    if not ffmpeg_path:
        logger.warning("‚ö†Ô∏è FFmpeg non trouv√© - pas de compression")
        return input_path, False

    # Taille originale
    original_size = os.path.getsize(input_path) / (1024 * 1024)  # MB

    # Si fichier d√©j√† petit (< 5 MB), pas besoin de compresser
    if original_size < 5:
        logger.info(f"‚è≠Ô∏è Fichier d√©j√† petit ({original_size:.1f}MB) - pas de compression n√©cessaire")
        return input_path, True

    logger.info(f"üîÑ COMPRESSION: {os.path.basename(input_path)} ({original_size:.1f}MB)")
    logger.info(f"   Param√®tres: 16kbps, 16kHz, mono")

    try:
        # Commande FFmpeg optimis√©e pour Whisper
        cmd = [
            ffmpeg_path,
            '-y',                    # √âcraser si existe
            '-i', input_path,        # Fichier source
            '-vn',                   # Pas de vid√©o
            '-ar', '16000',          # Sample rate 16kHz (natif Whisper)
            '-ac', '1',              # Mono
            '-b:a', '16k',           # Bitrate 16 kbps (optimal pour speech)
            '-acodec', 'libmp3lame', # Codec MP3
            output_path
        ]

        start_time = time.time()
        result = subprocess.run(
            cmd,
            capture_output=True,
            timeout=120,  # 2 minutes max pour compression
            encoding='utf-8',
            errors='ignore'
        )
        compression_time = time.time() - start_time

        if result.returncode == 0 and os.path.exists(output_path):
            compressed_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
            ratio = original_size / compressed_size if compressed_size > 0 else 0

            logger.info(f"‚úÖ COMPRESSION R√âUSSIE!")
            logger.info(f"   üìä {original_size:.1f}MB ‚Üí {compressed_size:.1f}MB (ratio {ratio:.1f}x)")
            logger.info(f"   ‚è±Ô∏è Temps: {compression_time:.1f}s")

            return output_path, True
        else:
            logger.error(f"‚ùå √âchec compression: {result.stderr[:200] if result.stderr else 'Erreur inconnue'}")
            return input_path, False

    except subprocess.TimeoutExpired:
        logger.error("‚ùå Timeout compression (>2min)")
        return input_path, False
    except Exception as e:
        logger.error(f"‚ùå Erreur compression: {e}")
        return input_path, False


# =============================================================================
# GESTIONNAIRE DE POOL D'API (Optimis√© pour 2 cl√©s)
# =============================================================================

class APIPoolManager:
    """
    Gestionnaire intelligent du pool de cl√©s API
    Optimis√© pour rotation entre 2 cl√©s API
    """

    def __init__(self, config):
        self.config = config
        self.api_keys = []
        self.clients = {}
        self.health_scores = {}
        self.usage_stats = {}
        self.lock = threading.RLock()
        self.current_key_index = 0

        # Charger les cl√©s depuis plusieurs sources
        self._load_api_keys()
        self._initialize_clients()

    def _load_api_keys(self):
        """Charge les cl√©s API depuis config et fichier externe"""
        keys_set = set()

        # 1. Charger la cl√© principale depuis config.ini
        try:
            main_key = self.config.get('API', 'openai_key', fallback='')
            if main_key and main_key.startswith('sk-'):
                keys_set.add(main_key)
                logger.info("‚úÖ Cl√© principale charg√©e depuis config.ini")
        except Exception as e:
            logger.warning(f"Erreur chargement config: {e}")

        # 2. Charger la cl√© depuis api_keys.txt (en √©vitant les doublons)
        api_keys_file = 'api_keys.txt'
        if os.path.exists(api_keys_file):
            try:
                with open(api_keys_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and line.startswith('sk-') and not line.startswith('#'):
                            keys_set.add(line)
                logger.info(f"‚úÖ Cl√©s charg√©es depuis {api_keys_file}")
            except Exception as e:
                logger.warning(f"Erreur lecture {api_keys_file}: {e}")

        self.api_keys = list(keys_set)

        logger.info("\n" + "="*70)
        logger.info("üîë D√âTECTION DES CL√âS API")
        logger.info("="*70)
        logger.info(f"üìã Nombre de cl√©s uniques trouv√©es: {len(self.api_keys)}")

        for i, key in enumerate(self.api_keys, 1):
            # Afficher les cl√©s de mani√®re s√©curis√©e
            logger.info(f"   Cl√© {i}: {key[:20]}...{key[-4:]}")

        if len(self.api_keys) == 0:
            raise ValueError("‚ùå ERREUR: Aucune cl√© API trouv√©e!")
        elif len(self.api_keys) == 1:
            logger.warning("‚ö†Ô∏è Une seule cl√© d√©tect√©e - Performance limit√©e")
            logger.warning("   ‚Üí 3 fichiers/minute maximum")
            logger.warning("   ‚Üí Ajoutez plus de cl√©s dans api_keys.txt pour am√©liorer")
        else:
            logger.info(f"‚úÖ {len(self.api_keys)} cl√©s actives!")
            logger.info(f"‚ö° Vitesse estim√©e: ~{len(self.api_keys) * 3} fichiers/minute")
        logger.info("="*70)

    def _initialize_clients(self):
        """Initialise un client OpenAI pour chaque cl√©"""
        for i, key in enumerate(self.api_keys):
            try:
                client = OpenAI(api_key=key)
                self.clients[key] = client
                self.health_scores[key] = 100  # Score initial
                self.usage_stats[key] = {
                    'requests': 0,
                    'successes': 0,
                    'failures': 0,
                    'last_used': None,
                    'rate_limited': False,
                    'rate_limit_reset': None
                }
                logger.debug(f"Client {i+1} initialis√©")
            except Exception as e:
                logger.error(f"√âchec init client {i+1}: {e}")

    def get_best_client(self) -> Tuple[Optional[str], Optional[OpenAI]]:
        """S√©lectionne le meilleur client disponible avec rotation intelligente"""
        with self.lock:
            if not self.api_keys:
                return None, None

            # Avec 1-2 cl√©s, strat√©gie simple de rotation
            attempts = 0
            while attempts < len(self.api_keys):
                key = self.api_keys[self.current_key_index]
                stats = self.usage_stats[key]

                # V√©rifier si la cl√© est rate limited
                if stats['rate_limited']:
                    if stats['rate_limit_reset'] and time.time() < stats['rate_limit_reset']:
                        # Encore en rate limit, essayer la suivante
                        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
                        attempts += 1
                        continue
                    else:
                        # Rate limit expir√©
                        stats['rate_limited'] = False
                        stats['rate_limit_reset'] = None

                # Cette cl√© est disponible
                stats['last_used'] = time.time()
                stats['requests'] += 1

                # Rotation pour la prochaine fois (si plusieurs cl√©s)
                if len(self.api_keys) > 1:
                    self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)

                return key, self.clients[key]

            # Aucune cl√© disponible
            logger.warning("‚ö†Ô∏è Toutes les cl√©s API sont rate limited!")
            return None, None

    def update_client_health(self, key: str, success: bool, error_type: str = None):
        """Met √† jour le score de sant√© d'une cl√©"""
        with self.lock:
            if key not in self.usage_stats:
                return

            stats = self.usage_stats[key]

            if success:
                stats['successes'] += 1
                self.health_scores[key] = min(100, self.health_scores[key] + 5)
            else:
                stats['failures'] += 1

                if error_type == 'rate_limit':
                    stats['rate_limited'] = True
                    stats['rate_limit_reset'] = time.time() + 60  # 1 minute
                    self.health_scores[key] -= 30
                elif error_type == 'quota_exceeded':
                    self.health_scores[key] = 0
                else:
                    self.health_scores[key] = max(0, self.health_scores[key] - 10)

    def get_stats(self) -> Dict:
        """Retourne les statistiques du pool"""
        with self.lock:
            total_requests = sum(s['requests'] for s in self.usage_stats.values())
            total_successes = sum(s['successes'] for s in self.usage_stats.values())
            active_keys = sum(1 for k in self.api_keys if self.health_scores.get(k, 0) > 0)

            return {
                'total_keys': len(self.api_keys),
                'active_keys': active_keys,
                'total_requests': total_requests,
                'total_successes': total_successes,
                'success_rate': (total_successes / total_requests * 100) if total_requests > 0 else 0
            }

# =============================================================================
# CHUNKER INTELLIGENT (R√©sout le probl√®me des fichiers >25MB)
# =============================================================================

class SmartChunker:
    """
    Syst√®me de chunking adaptatif pour fichiers audio
    R√©sout le probl√®me critique des fichiers >25MB qui causent 30% d'erreurs
    """

    def __init__(self, config):
        self.config = config
        self.ffmpeg_path = self._get_ffmpeg_path()
        self.max_size_mb = 20  # Limite de s√©curit√© (OpenAI = 25MB)
        self.target_chunk_size_mb = 10  # Taille cible des chunks
        self.temp_dir = os.path.join(os.getcwd(), 'temp_chunks')

    def _get_ffmpeg_path(self):
        """Trouve le chemin FFmpeg depuis la config"""
        # D'abord essayer depuis la config
        ffmpeg_path = self.config.get('Conversion', 'ffmpeg_path',
                                     fallback='ffmpeg')

        # V√©rifier si le chemin existe
        if os.path.exists(ffmpeg_path):
            return ffmpeg_path

        # Essayer le dossier ffmpeg local
        local_ffmpeg = os.path.join(os.getcwd(), 'ffmpeg', 'bin', 'ffmpeg.exe')
        if os.path.exists(local_ffmpeg):
            return local_ffmpeg

        # Fallback sur ffmpeg dans PATH
        return 'ffmpeg'

    def needs_chunking(self, file_path: str) -> bool:
        """D√©termine si un fichier n√©cessite un chunking"""
        try:
            size_mb = os.path.getsize(file_path) / (1024 * 1024)
            return size_mb > self.max_size_mb
        except:
            return False

    def chunk_audio(self, file_path: str) -> List[str]:
        """
        D√©coupe un fichier audio en chunks intelligents
        Retourne la liste des chemins des chunks
        """
        if not self.needs_chunking(file_path):
            return [file_path]

        logger.info(f"üî™ Chunking n√©cessaire pour {os.path.basename(file_path)}")

        # Cr√©er le dossier temporaire
        os.makedirs(self.temp_dir, exist_ok=True)

        # Nom de base pour les chunks
        base_name = os.path.splitext(os.path.basename(file_path))[0]

        # Calculer le nombre de chunks n√©cessaires
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        num_chunks = int(file_size_mb / self.target_chunk_size_mb) + 1

        chunks = []

        # Cr√©er les chunks avec FFmpeg
        for i in range(num_chunks):
            chunk_path = os.path.join(self.temp_dir, f"{base_name}_chunk_{i:03d}.mp3")

            # Commande FFmpeg pour extraire un segment
            # Utilise la segmentation par dur√©e approximative
            cmd = [
                self.ffmpeg_path,
                '-i', file_path,
                '-f', 'segment',
                '-segment_time', str(300),  # 5 minutes par chunk
                '-c', 'copy',
                '-segment_start_number', str(i),
                '-vn',  # Pas de vid√©o
                chunk_path
            ]

            try:
                # Alternative: d√©coupage simple par taille
                # Pour √©viter les probl√®mes de segmentation
                duration_per_chunk = self._get_duration_for_chunks(file_path, num_chunks)

                cmd = [
                    self.ffmpeg_path,
                    '-i', file_path,
                    '-ss', str(i * duration_per_chunk),
                    '-t', str(duration_per_chunk),
                    '-acodec', 'mp3',
                    '-ab', '128k',
                    '-y',
                    chunk_path
                ]

                result = subprocess.run(cmd, capture_output=True, text=True,
                                      encoding='utf-8', errors='ignore')

                if os.path.exists(chunk_path) and os.path.getsize(chunk_path) > 0:
                    chunks.append(chunk_path)
                    logger.info(f"  ‚úÖ Chunk {i+1}/{num_chunks} cr√©√©")

            except Exception as e:
                logger.error(f"Erreur cr√©ation chunk {i}: {e}")

        return chunks if chunks else [file_path]

    def _get_duration_for_chunks(self, file_path: str, num_chunks: int) -> float:
        """Calcule la dur√©e approximative par chunk"""
        try:
            # Commande pour obtenir la dur√©e
            cmd = [
                self.ffmpeg_path,
                '-i', file_path,
                '-hide_banner'
            ]

            result = subprocess.run(cmd, capture_output=True, text=True,
                                  encoding='utf-8', errors='ignore', timeout=5)

            # Parser la dur√©e depuis stderr
            import re
            duration_match = re.search(r'Duration: (\d{2}):(\d{2}):(\d{2})', result.stderr)
            if duration_match:
                hours = int(duration_match.group(1))
                minutes = int(duration_match.group(2))
                seconds = int(duration_match.group(3))
                total_duration = hours * 3600 + minutes * 60 + seconds
                return total_duration / num_chunks
        except:
            pass

        # Fallback: 5 minutes par chunk
        return 300

    def cleanup_chunks(self):
        """Nettoie tous les fichiers chunks temporaires"""
        try:
            if os.path.exists(self.temp_dir):
                import shutil
                shutil.rmtree(self.temp_dir)
                logger.debug("Chunks temporaires nettoy√©s")
        except Exception as e:
            logger.warning(f"Erreur nettoyage chunks: {e}")

# =============================================================================
# CACHE DE TRANSCRIPTIONS
# =============================================================================

class TranscriptionCache:
    """
    Cache LRU pour les transcriptions
    √âvite de re-transcrire les m√™mes fichiers
    """

    def __init__(self, max_size_mb=2000):  # Augment√© √† 2GB pour √©viter toute suppression de cache
        self.max_size_mb = max_size_mb  # Stocker pour l'affichage
        self.max_size_bytes = max_size_mb * 1024 * 1024
        self.cache = OrderedDict()
        self.cache_file = '.transcription_cache.json'
        self.current_size = 0
        self.hits = 0
        self.misses = 0
        self.lock = threading.RLock()

        # Charger le cache existant
        self._load_cache()

    def _load_cache(self):
        """Charge le cache depuis le disque"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # Limiter la taille lors du chargement (augment√© 100‚Üí500 pour g√©rer 373 entr√©es actuelles)
                    for key, value in list(data.items())[:500]:
                        self.cache[key] = value
                        self.current_size += len(value.get('text', '').encode('utf-8'))
                logger.info(f"üì¶ [CACHE] Charg√©: {len(self.cache)} entr√©es ({self.current_size / (1024*1024):.2f} MB)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [CACHE] Erreur chargement ({e}), cr√©ation nouveau cache")

    def get(self, file_hash: str) -> Optional[str]:
        """R√©cup√®re une transcription du cache"""
        with self.lock:
            if file_hash in self.cache:
                self.hits += 1
                self.cache.move_to_end(file_hash)  # LRU
                return self.cache[file_hash].get('text')
            self.misses += 1
            return None

    def put(self, file_hash: str, transcription: str):
        """Ajoute une transcription au cache"""
        with self.lock:
            entry_size = len(transcription.encode('utf-8'))

            # √âviction LRU si n√©cessaire
            while self.current_size + entry_size > self.max_size_bytes and self.cache:
                oldest_key = next(iter(self.cache))
                oldest_size = len(self.cache[oldest_key].get('text', '').encode('utf-8'))
                del self.cache[oldest_key]
                self.current_size -= oldest_size

            # Ajouter la nouvelle entr√©e
            self.cache[file_hash] = {
                'text': transcription,
                'timestamp': datetime.now().isoformat()
            }
            self.current_size += entry_size

    def save(self):
        """Sauvegarde le cache sur disque"""
        try:
            with self.lock:
                with open(self.cache_file, 'w', encoding='utf-8') as f:
                    json.dump(dict(self.cache), f, ensure_ascii=False)
        except Exception as e:
            logger.error(f"Erreur sauvegarde cache: {e}")

    def get_stats(self) -> Dict:
        """Retourne les statistiques du cache"""
        with self.lock:
            hit_rate = (self.hits / (self.hits + self.misses) * 100) if (self.hits + self.misses) > 0 else 0
            return {
                'entries': len(self.cache),
                'size_mb': self.current_size / (1024 * 1024),
                'hits': self.hits,
                'misses': self.misses,
                'hit_rate': hit_rate
            }

# =============================================================================
# CLASSE PRINCIPALE TRANSCRIBER ULTRA
# =============================================================================

class TranscriberUltra:
    """
    Transcripteur Ultra-Optimis√© pour WhatsApp Extractor V2
    Drop-in replacement avec interface 100% compatible
    Optimis√© pour 2 cl√©s API avec chunking intelligent et cache
    """

    def __init__(self, config, registry, file_manager):
        """
        Initialisation IDENTIQUE aux autres transcripteurs
        Garantit la compatibilit√© totale avec le syst√®me existant
        """
        self.config = config
        self.registry = registry
        self.file_manager = file_manager

        # Logger
        self.logger = logging.getLogger('whatsapp_extractor')

        # Composants d'optimisation
        self.api_pool = APIPoolManager(config)
        self.chunker = SmartChunker(config)
        self.cache = TranscriptionCache()

        # Configuration optimis√©e pour 2 cl√©s
        # Avec 2 cl√©s: 4-6 workers optimaux
        self.max_workers = min(6, len(self.api_pool.api_keys) * 3)
        self.max_transcriptions = config.getint('Processing', 'max_transcriptions', fallback=0)
        
        # Mode de transcription forc√©e (pour compatibilit√©)
        self.force_transcription = config.getboolean('Processing', 'force_transcription', fallback=False)

        # DEDUPLICATION: Flag pour activer/d√©sactiver la d√©duplication des sources
        self.enable_source_deduplication = config.getboolean('Processing', 'enable_source_deduplication', fallback=True)

        # Statistiques
        self.stats = {
            'start_time': 0,
            'files_processed': 0,
            'files_cached': 0,
            'files_skipped': 0,
            'files_assembled_from_cache': 0,  # DEDUP: SuperFiles assembl√©s depuis cache
            'source_cache_hits': 0,            # DEDUP: Nombre de sources trouv√©es en cache
            'chunks_created': 0,
            'api_calls': 0,
            'errors': 0
        }

        logger.info("\n" + "="*70)
        logger.info("‚ö° TRANSCRIBER ULTRA INITIALIS√â - MODE OPTIMISATION MAXIMALE")
        logger.info("="*70)
        logger.info(f"üîë Cl√©s API d√©tect√©es: {len(self.api_pool.api_keys)}")
        if len(self.api_pool.api_keys) == 1:
            logger.warning("‚ö†Ô∏è  Une seule cl√© d√©tect√©e - Performance limit√©e √† 3 fichiers/minute")
        else:
            logger.info(f"‚úÖ {len(self.api_pool.api_keys)} cl√©s actives - Performance x{len(self.api_pool.api_keys)}")
        logger.info(f"üë∑ Workers parall√®les: {self.max_workers}")
        logger.info(f"üíæ Cache: Activ√© ({self.cache.max_size_mb}MB)")
        logger.info(f"üî™ Chunking: Activ√© pour fichiers >20MB")
        logger.info(f"üéØ D√©duplication sources: {'Activ√©' if self.enable_source_deduplication else 'D√©sactiv√©'}")
        logger.info("="*70)

    def transcribe_all(self, conversations: Dict[str, List[Dict]]) -> Dict[str, int]:
        """
        M√âTHODE PRINCIPALE - Interface IDENTIQUE pour compatibilit√©
        Cette m√©thode est appel√©e par le workflow principal
        """
        self.stats['start_time'] = time.time()

        logger.info("\n" + "#"*70)
        logger.info("# üöÄ D√âBUT TRANSCRIPTION ULTRA")
        logger.info("#"*70)
        logger.info(f"üîë Cl√©s API: {len(self.api_pool.api_keys)}")
        logger.info(f"üìÇ Conversations: {len(conversations)} contacts")

        # LOGS D√âTAILL√âS POUR D√âBUGGER
        logger.info("\nüî• MODE DEBUG TRANSCRIPTION")
        logger.info(f"üìç Registry path: {self.registry.registry_path}")
        if os.path.exists(self.registry.registry_path):
            with open(self.registry.registry_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                trans_count = len(data.get('transcriptions', {}))
                logger.info(f"   üìä Transcriptions existantes dans registry: {trans_count}")
        else:
            logger.info(f"   ‚ö†Ô∏è Registry VIDE - Toutes les transcriptions seront faites!")

        # Logger les contacts pour debug
        for contact_name in conversations.keys():
            logger.info(f"   Contact: {contact_name}")

        logger.info("\n" + "="*70)
        logger.info("üìã PHASE 1: D√âCOUVERTE DES FICHIERS")
        logger.info("="*70)

        # Extraire tous les fichiers SuperAudio √† traiter
        super_files = self._get_super_files_from_conversations(conversations)

        if not super_files:
            logger.warning("‚ùå Aucun fichier SuperAudio/SUPER_FICHIERS trouv√© √† transcrire")
            logger.warning("V√©rifiez que les fichiers audio ont √©t√© cr√©√©s dans l'√©tape pr√©c√©dente")
            logger.warning("Certains contacts avec caract√®res sp√©ciaux peuvent avoir √©chou√©")
            # Ne pas bloquer compl√®tement, continuer avec ce qu'on a
            return {'transcribed': 0, 'failed': 0, 'warning': 'Aucun fichier trouv√©'}

        logger.info(f"\nüìÅ R√âSULTAT: {len(super_files)} fichiers SuperAudio trouv√©s au total")

        logger.info("\n" + "="*70)
        logger.info("üìã PHASE 2: FILTRAGE ET PR√âPARATION AVEC D√âDUPLICATION")
        logger.info("="*70)

        # Filtrer les fichiers avec d√©duplication avanc√©e
        # Retourne: (to_transcribe, to_assemble)
        files_to_transcribe, files_to_assemble = self._filter_already_transcribed(super_files)

        total_to_process = len(files_to_transcribe) + len(files_to_assemble)
        logger.info(f"üéØ {len(files_to_transcribe)} fichiers √† transcrire (API)")
        logger.info(f"üîÑ {len(files_to_assemble)} fichiers √† assembler (cache sources)")
        logger.info(f"‚è≠Ô∏è  {len(super_files) - total_to_process} fichiers d√©j√† transcrits (skip)")

        if not files_to_transcribe and not files_to_assemble:
            logger.info("‚ú® Tous les fichiers sont d√©j√† transcrits!")
            return {'transcribed': 0, 'failed': 0, 'skipped': len(super_files)}

        # Limiter si configur√© (seulement les fichiers √† transcrire)
        if self.max_transcriptions > 0 and len(files_to_transcribe) > self.max_transcriptions:
            logger.info(f"‚ö†Ô∏è Limitation √† {self.max_transcriptions} fichiers")
            files_to_transcribe = files_to_transcribe[:self.max_transcriptions]

        # Optimiser l'ordre de traitement (petits fichiers d'abord)
        files_to_transcribe = self._optimize_processing_order(files_to_transcribe)

        # Pr√©parer les √©l√©ments de travail avec chunking si n√©cessaire
        work_items = self._prepare_work_items(files_to_transcribe)
        logger.info(f"üì¶ {len(work_items)} √©l√©ments de travail pr√©par√©s pour transcription")

        # Ex√©cuter les transcriptions en parall√®le (Phase 1) puis assemblage (Phase 2)
        results = self._execute_parallel_transcription(work_items, files_to_assemble)

        # Sauvegarder le cache
        self.cache.save()

        # Nettoyer les chunks temporaires
        self.chunker.cleanup_chunks()

        # Statistiques finales
        self._print_statistics()

        # Retour compatible avec le syst√®me existant
        return {
            'transcribed': self.stats['files_processed'],
            'failed': self.stats['errors'],
            'cached': self.stats['files_cached'],
            'skipped': self.stats['files_skipped']
        }

    def _create_normalized_super_files(self, contact: str, contact_dir: str) -> List[str]:
        """
        Cr√©e des SuperFiles avec noms normalis√©s pour contourner les probl√®mes d'encodage
        """
        logger.info(f"üîß [NORMALIZE] Cr√©ation SuperFiles normalis√©s pour {contact}")

        # Obtenir le nom normalis√©
        normalized_name = normalize_contact_name_for_filesystem(contact)

        logger.info(f"üìù [NORMALIZE] '{contact}' ‚Üí '{normalized_name}'")

        # Chercher les fichiers MP3 individuels dans le dossier audio
        audio_dirs = [
            os.path.join(contact_dir, 'media_recus', 'audio'),
            os.path.join(contact_dir, 'audio'),
            os.path.join(contact_dir, 'Audio')
        ]

        mp3_files = []
        for audio_dir in audio_dirs:
            if os.path.exists(audio_dir):
                logger.info(f"üîç [NORMALIZE] Scan audio: {audio_dir}")
                for file in os.listdir(audio_dir):
                    if file.endswith('.mp3'):
                        mp3_path = os.path.join(audio_dir, file)
                        if os.path.isfile(mp3_path):
                            mp3_files.append(mp3_path)

        if not mp3_files:
            logger.warning(f"‚ùå [NORMALIZE] Aucun fichier MP3 trouv√© pour {contact}")
            return []

        logger.info(f"üìä [NORMALIZE] {len(mp3_files)} fichiers MP3 trouv√©s")

        # Cr√©er le dossier SuperAudio normalis√©
        super_audio_dir = os.path.join(contact_dir, 'SuperAudio')
        os.makedirs(super_audio_dir, exist_ok=True)

        # Cr√©er un SuperFile avec nom normalis√©
        super_file_path = os.path.join(super_audio_dir, f"{normalized_name}_received_2025-09.mp3")

        try:
            # Cr√©er le fichier liste pour FFmpeg avec chemins corrects
            filelist_path = super_file_path + ".txt"
            with open(filelist_path, 'w', encoding='utf-8') as f:
                for mp3_file in mp3_files[:15]:  # Limiter √† 15 fichiers pour √©viter les gros SuperFiles
                    # √âchapper les chemins pour FFmpeg
                    escaped_path = mp3_file.replace('\\', '/')
                    f.write(f"file '{escaped_path}'\n")

            # NOUVELLE APPROCHE : Utiliser Python pour concat√©ner les MP3 directement
            logger.info(f"üöÄ [NORMALIZE] Cr√©ation SuperFile: {os.path.basename(super_file_path)}")

            try:
                # Concat√©ner les MP3 avec Python (plus simple que FFmpeg)
                with open(super_file_path, 'wb') as outfile:
                    for mp3_file in mp3_files[:15]:  # Limiter √† 15 fichiers
                        if os.path.exists(mp3_file):
                            with open(mp3_file, 'rb') as infile:
                                outfile.write(infile.read())
                            logger.debug(f"   Ajout√©: {os.path.basename(mp3_file)}")

                # V√©rifier que le fichier est cr√©√©
                if os.path.exists(super_file_path) and os.path.getsize(super_file_path) > 0:
                    file_size = os.path.getsize(super_file_path) / (1024*1024)
                    logger.info(f"‚úÖ [NORMALIZE] SuperFile cr√©√© avec succ√®s: {os.path.basename(super_file_path)} ({file_size:.2f}MB)")

                    # Nettoyer le fichier liste
                    try:
                        os.remove(filelist_path)
                    except:
                        pass

                    return [super_file_path]
                else:
                    logger.error(f"‚ùå [NORMALIZE] √âchec cr√©ation fichier vide")
                    return []

            except Exception as e:
                logger.error(f"‚ùå [NORMALIZE] Erreur concat√©nation: {e}")
                return []

        except Exception as e:
            logger.error(f"‚ùå [NORMALIZE] Erreur cr√©ation SuperFile: {e}")
            return []

    def _get_super_files_from_conversations(self, conversations: Dict) -> List[str]:
        """Extrait tous les fichiers SuperAudio des conversations"""
        super_files = []

        logger.info(f"‚ö° [ULTRA] Recherche des SuperFiles pour {len(conversations)} contacts")

        for contact in conversations.keys():
            logger.info(f"üìÇ [ULTRA] Traitement contact: {contact}")

            try:
                # üö® NORMALISATION AUTOMATIQUE - Z√âRO PERTE DE LEADS
                # Chercher les SuperFiles dans BOTH r√©pertoires: original ET normalis√©
                safe_contact = normalize_contact_name_for_filesystem(contact)
                
                # Construire le chemin vers le dossier SuperAudio du contact ORIGINAL
                paths = self.file_manager.setup_contact_directory(contact)
                contact_dir = paths['root']
                
                # Construire AUSSI le chemin vers le dossier SuperAudio NORMALIS√â
                safe_paths = self.file_manager.setup_contact_directory(safe_contact)
                safe_contact_dir = safe_paths['root']

                logger.info(f"üìç [ULTRA] R√©pertoire contact original: {contact_dir}")
                if safe_contact != contact:
                    logger.info(f"üìç [ULTRA] R√©pertoire contact normalis√©: {safe_contact_dir}")

                # Chercher dans TOUS les r√©pertoires possibles (original + normalis√©)
                dirs_to_check = [
                    # R√©pertoires avec nom original
                    os.path.join(contact_dir, 'SuperAudio'),
                    os.path.join(contact_dir, 'SUPER_FICHIERS'),
                    paths.get('super_files', ''),
                    paths.get('SuperAudio', ''),
                    # R√©pertoires avec nom normalis√© (pour les nouveaux SuperFiles)
                    os.path.join(safe_contact_dir, 'SuperAudio'),
                    os.path.join(safe_contact_dir, 'SUPER_FICHIERS'),
                    safe_paths.get('super_files', ''),
                    safe_paths.get('SuperAudio', '')
                ]

                files_found_for_contact = 0
                for super_dir in dirs_to_check:
                    if super_dir and os.path.exists(super_dir):
                        logger.info(f"üîç [ULTRA] Scan du dossier: {super_dir}")
                        logger.info(f"   Dossier existe: {os.path.isdir(super_dir)}")
                        try:
                            files_in_dir = os.listdir(super_dir)
                            logger.info(f"üìä [ULTRA] {len(files_in_dir)} fichiers trouv√©s dans {os.path.basename(super_dir)}")

                            # Logger tous les fichiers pour debug
                            for f in files_in_dir:
                                logger.debug(f"   Fichier trouv√©: {f}")

                            # FILTRE DIRECTION: V√©rifier si on doit transcrire les fichiers envoy√©s
                            transcribe_sent = self.config.getboolean('Processing', 'transcribe_sent', fallback=False)

                            for file in files_in_dir:
                                if file.endswith('.mp3') and not file.endswith('.mp3.txt'):
                                    # NOUVEAU: Filtrer par direction si transcribe_sent=False
                                    if not transcribe_sent and '_sent_' in file.lower():
                                        logger.info(f"[SKIP] Fichier SENT ignor√© (transcribe_sent=False): {file}")
                                        continue

                                    file_path = os.path.join(super_dir, file)
                                    # V√©rifier que le fichier existe r√©ellement
                                    if os.path.isfile(file_path):
                                        if file_path not in super_files:  # √âviter doublons
                                            super_files.append(file_path)
                                            files_found_for_contact += 1
                                            try:
                                                size_mb = os.path.getsize(file_path) / (1024*1024)
                                                logger.info(f"‚úÖ [ULTRA] SuperFile ajout√©: {file} ({size_mb:.2f}MB)")
                                                logger.info(f"   Chemin complet: {file_path}")
                                            except:
                                                logger.info(f"‚úÖ [ULTRA] SuperFile ajout√©: {file}")
                                    else:
                                        logger.warning(f"   Fichier MP3 trouv√© mais pas accessible: {file_path}")
                        except Exception as e:
                            logger.error(f"‚ùå [ULTRA] Erreur lecture {super_dir}: {e}")
                            import traceback
                            logger.error(traceback.format_exc())

                if files_found_for_contact == 0:
                    logger.warning(f"‚ö†Ô∏è [ULTRA] Aucun SuperFile trouv√© pour {contact}")

                    # NOUVELLE STRAT√âGIE: Cr√©er des SuperFiles avec noms normalis√©s
                    logger.info(f"üîß [ULTRA] Tentative de cr√©ation SuperFile normalis√© pour {contact}")
                    try:
                        created_files = self._create_normalized_super_files(contact, contact_dir)
                        if created_files:
                            super_files.extend(created_files)
                            logger.info(f"‚úÖ [ULTRA] {len(created_files)} SuperFiles cr√©√©s avec nom normalis√©!")
                        else:
                            logger.warning(f"‚ùå [ULTRA] Impossible de cr√©er SuperFile pour {contact}")
                    except Exception as create_error:
                        logger.error(f"‚ùå [ULTRA] √âchec cr√©ation SuperFile normalis√©: {create_error}")
                else:
                    logger.info(f"‚úÖ [ULTRA] {files_found_for_contact} SuperFiles trouv√©s pour {contact}")
            except Exception as e:
                logger.error(f"‚ùå [ULTRA] Erreur traitement contact {contact}: {e}")
                logger.warning(f"‚ö†Ô∏è [ULTRA] Contact ignor√© √† cause de l'erreur (caract√®res sp√©ciaux?)")
                # Continuer avec les autres contacts au lieu de bloquer
                continue

        logger.info(f"‚úÖ [ULTRA] {len(super_files)} SuperFiles trouv√©s pour tous les contacts")
        logger.info(f"\nüìÅ R√âSULTAT: {len(super_files)} fichiers SuperAudio trouv√©s au total")
        
        return super_files

    def _filter_already_transcribed(self, files: List[str]) -> Tuple[List[str], List[str]]:
        """
        Filtre les fichiers d√©j√† transcrits avec d√©duplication avanc√©e.

        Retourne:
            Tuple (to_transcribe, to_assemble):
            - to_transcribe: SuperFiles qui doivent √™tre envoy√©s √† l'API Whisper
            - to_assemble: SuperFiles dont toutes les sources sont en cache (assemblage local)
        """
        logger.info("\n" + "="*70)
        logger.info("üîç [FILTRAGE] ANALYSE DES SUPERFILES √Ä TRANSCRIRE")
        logger.info("="*70)

        # ============================================================
        # √âTAPE 1: Filtrage classique (cache, registry, √©checs)
        # ============================================================
        candidates = []  # Fichiers qui passent le filtrage classique

        for file_path in files:
            file_hash = self.registry.get_file_hash(file_path)
            file_name = os.path.basename(file_path)
            try:
                file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
            except:
                file_size_mb = 0

            logger.debug(f"üìÑ Analyse: {file_name} ({file_size_mb:.2f} MB)")

            # Mode forc√© : toujours transcrire
            if self.force_transcription:
                logger.debug("‚ö° [MODE FORC√â] Transcription forc√©e activ√©e")
                candidates.append(file_path)
                continue

            # V√©rifier si c'est un SuperFile incr√©mental (nouvellement cr√©√©)
            is_incremental_file = self._is_incremental_superfile(file_path)
            if is_incremental_file:
                logger.debug("üîÑ [INCR√âMENTAL] SuperFile nouvellement cr√©√© d√©tect√©")
                candidates.append(file_path)
                continue

            # V√©rifier le cache local d'abord
            cached_transcription = self.cache.get(file_hash)
            if cached_transcription:
                logger.debug("üíæ [CACHE HIT] Transcription trouv√©e dans le cache")
                self.stats['files_cached'] += 1
                continue

            # V√©rifier le Registry
            registry_transcription = self.registry.get_transcription(file_hash)
            if registry_transcription:
                logger.debug("üóÉÔ∏è [REGISTRY HIT] Transcription trouv√©e dans le registry")
                self.cache.put(file_hash, registry_transcription)
                self.stats['files_cached'] += 1

                # ‚úÖ NOUVEAU : Cr√©er les fichiers .txt et .segments.json si absents
                sf_path = work_item['path']
                txt_file = sf_path + '.txt'
                segments_file = sf_path + '.segments.json'

                # Cr√©er .txt si absent
                if not os.path.exists(txt_file):
                    with open(txt_file, 'w', encoding='utf-8') as f:
                        f.write(registry_transcription)
                    logger.debug(f"üìù [REGISTRY] Fichier .txt recr√©√© depuis registry")

                # Cr√©er segments.json si absent
                if not os.path.exists(segments_file):
                    source_details = self._get_superfile_source_info(sf_path)
                    segments_data = {
                        'file': os.path.basename(sf_path),
                        'total_duration': 0,
                        'segments_count': 0,
                        'segments': [],
                        'from_registry_cache': True
                    }

                    if source_details:
                        segments_data['source_files_details'] = source_details
                        total_dur = sum(s.get('duration', 0) for s in source_details)
                        segments_data['total_duration'] = total_dur
                        logger.debug(f"üìä [REGISTRY] segments.json recr√©√© avec {len(source_details)} sources")

                    with open(segments_file, 'w', encoding='utf-8') as f:
                        import json
                        json.dump(segments_data, f, ensure_ascii=False, indent=2)

                continue

            # V√©rifier si √©chec pr√©c√©dent
            if not self.registry.should_retry_transcription(file_hash):
                logger.debug("‚ùå [SKIP] Trop d'√©checs pr√©c√©dents, ignor√©")
                self.stats['files_skipped'] += 1
                continue

            # Fichier candidat pour transcription
            candidates.append(file_path)

        logger.info(f"üìã {len(candidates)} candidats apr√®s filtrage classique (sur {len(files)} total)")

        # ============================================================
        # √âTAPE 2: D√©duplication avanc√©e (si activ√©e)
        # ============================================================
        if not self.enable_source_deduplication or not candidates:
            # D√©duplication d√©sactiv√©e ou pas de candidats
            logger.info("‚è≠Ô∏è D√©duplication d√©sactiv√©e ou pas de candidats")
            return candidates, []

        # Construire l'index des sources pour les candidats
        hash_index = self._build_source_hash_index(candidates)

        # Classifier les SuperFiles
        to_transcribe, to_assemble = self._classify_superfiles(candidates, hash_index)

        # ============================================================
        # √âTAPE 3: R√©sum√© final
        # ============================================================
        logger.info("\n" + "="*60)
        logger.info("üìä R√âSULTAT DU FILTRAGE AVEC D√âDUPLICATION")
        logger.info("="*60)
        logger.info(f"   üì• Fichiers analys√©s: {len(files)}")
        logger.info(f"   üíæ Depuis cache SuperFile: {self.stats['files_cached']}")
        logger.info(f"   ‚è≠Ô∏è Ignor√©s (√©checs): {self.stats['files_skipped']}")
        logger.info(f"   üöÄ √Ä TRANSCRIRE (API): {len(to_transcribe)}")
        logger.info(f"   üîÑ √Ä ASSEMBLER (cache sources): {len(to_assemble)}")

        if to_transcribe:
            logger.info(f"\nüìã FICHIERS QUI VONT √äTRE TRANSCRITS:")
            for i, f in enumerate(to_transcribe, 1):
                logger.info(f"   {i}. {os.path.basename(f)}")

        if to_assemble:
            logger.info(f"\nüìã FICHIERS QUI VONT √äTRE ASSEMBL√âS DEPUIS CACHE:")
            for i, f in enumerate(to_assemble, 1):
                logger.info(f"   {i}. {os.path.basename(f)}")

        if not to_transcribe and not to_assemble:
            logger.warning("\n‚ö†Ô∏è AUCUN FICHIER √Ä TRAITER!")

        logger.info("="*70)
        return to_transcribe, to_assemble

    def _get_superfile_source_info(self, file_path: str) -> List[Dict]:
        """
        R√©cup√®re les informations sur les fichiers sources d'un SuperFile.
        Utilis√© pour la d√©duplication au niveau des fichiers sources.

        G√®re les SuperFiles multi-parties (ex: contact_received_2025-01_part2.mp3)

        Args:
            file_path: Chemin du SuperFile

        Returns:
            Liste de dicts avec {filename, hash, duration} pour chaque fichier source
        """
        try:
            # Extraire le contact et la p√©riode du nom du fichier
            file_name = os.path.basename(file_path)
            base_name = file_name.replace('.mp3', '')
            parts = base_name.split('_')

            # Format attendu:
            # - Sans partie: contact_direction_YYYY-MM.mp3
            # - Avec partie: contact_direction_YYYY-MM_part2.mp3
            if len(parts) >= 3:
                # Trouver la direction, p√©riode et partie
                direction = None
                period = None
                part_suffix = ""
                contact_parts = []

                for i, part in enumerate(parts):
                    if part in ['received', 'sent', 'unknown']:
                        direction = part
                        # Le reste avant direction est le contact
                        contact_parts = parts[:i]
                    elif len(part) == 7 and '-' in part:  # YYYY-MM
                        period = part
                    elif part.startswith('part') and len(part) >= 5:  # part1, part2, etc.
                        # Extraire le suffixe de partie (ex: _part2)
                        part_suffix = f"_{part}"

                if direction and period:
                    contact = '_'.join(contact_parts) if contact_parts else parts[0]
                    # Inclure le suffixe de partie dans la p√©riode pour la cl√© du registry
                    registry_period = period + part_suffix

                    # R√©cup√©rer les infos depuis le registry
                    super_file_info = self.registry.get_super_file_info(contact, registry_period, direction)
                    if super_file_info:
                        source_details = super_file_info.get('source_files_details', [])
                        if source_details:
                            # Ajouter les hash SHA256 pour chaque fichier source si pas d√©j√† pr√©sent
                            for detail in source_details:
                                if 'hash' not in detail and 'mp3_path' in detail:
                                    mp3_path = detail['mp3_path']
                                    if os.path.exists(mp3_path):
                                        detail['hash'] = self.registry.get_file_hash(mp3_path)

                            logger.debug(f"[SOURCE INFO] {len(source_details)} fichiers sources pour {file_name}")
                            return source_details
                    else:
                        # Fallback: essayer sans le suffixe de partie (anciens fichiers)
                        if part_suffix:
                            super_file_info = self.registry.get_super_file_info(contact, period, direction)
                            if super_file_info:
                                source_details = super_file_info.get('source_files_details', [])
                                if source_details:
                                    logger.debug(f"[SOURCE INFO] Fallback sans partie: {len(source_details)} sources")
                                    return source_details

            return []
        except Exception as e:
            logger.debug(f"[SOURCE INFO] Erreur r√©cup√©ration info sources: {e}")
            return []

    def _cache_transcription_by_source_files(self, superfile_path: str, full_transcription: str,
                                               segments: List[Dict] = None):
        """
        Cache la transcription par hash de chaque fichier source.
        Utilise les timestamps des segments pour segmenter la transcription.

        Args:
            superfile_path: Chemin du SuperFile transcrit
            full_transcription: Texte complet de la transcription
            segments: Segments avec timestamps de Whisper API (optionnel)
        """
        try:
            source_info = self._get_superfile_source_info(superfile_path)
            if not source_info:
                logger.debug("[SOURCE CACHE] Pas d'info source pour ce SuperFile")
                return

            logger.info(f"üîÑ [SOURCE CACHE] Mise en cache pour {len(source_info)} fichiers sources")

            # Si on a des segments avec timestamps, on peut segmenter pr√©cis√©ment
            if segments and len(segments) > 0:
                # Construire un mapping temps -> texte
                current_time = 0
                source_idx = 0

                for source in source_info:
                    source_duration = source.get('duration', 0)
                    source_end_time = current_time + source_duration

                    # Collecter les segments qui appartiennent √† ce fichier source
                    source_text_parts = []
                    for seg in segments:
                        seg_start = seg.get('start', 0)
                        seg_end = seg.get('end', 0)

                        # Si le segment chevauche la plage du fichier source
                        if seg_start < source_end_time and seg_end > current_time:
                            source_text_parts.append(seg.get('text', ''))

                    source_text = ' '.join(source_text_parts).strip()

                    # Obtenir le hash du fichier source original (pas le MP3 converti)
                    source_hash = source.get('hash')
                    original_file = source.get('original_path') or source.get('filename', '')

                    # Si pas de hash mais on a le chemin MP3, calculer le hash
                    if not source_hash and source.get('mp3_path'):
                        mp3_path = source['mp3_path']
                        if os.path.exists(mp3_path):
                            source_hash = self.registry.get_file_hash(mp3_path)

                    if source_hash and source_text:
                        self.registry.register_source_transcription(
                            source_hash=source_hash,
                            transcription_text=source_text,
                            duration_seconds=source_duration,
                            original_file=original_file
                        )
                        logger.debug(f"   ‚úì Cach√©: {original_file[:30]}... ({len(source_text)} chars)")

                    current_time = source_end_time
                    source_idx += 1

            else:
                # Pas de segments - diviser le texte proportionnellement
                # Moins pr√©cis mais mieux que rien
                total_duration = sum(s.get('duration', 0) for s in source_info)
                if total_duration <= 0:
                    logger.debug("[SOURCE CACHE] Dur√©e totale = 0, skip")
                    return

                text_length = len(full_transcription)
                current_pos = 0

                for source in source_info:
                    source_duration = source.get('duration', 0)
                    proportion = source_duration / total_duration if total_duration > 0 else 0
                    chars_for_source = int(text_length * proportion)

                    # Extraire la portion de texte (essayer de couper aux espaces)
                    end_pos = min(current_pos + chars_for_source, text_length)
                    if end_pos < text_length:
                        # Chercher le prochain espace pour couper proprement
                        while end_pos < text_length and full_transcription[end_pos] != ' ':
                            end_pos += 1

                    source_text = full_transcription[current_pos:end_pos].strip()

                    # Obtenir le hash
                    source_hash = source.get('hash')
                    original_file = source.get('original_path') or source.get('filename', '')

                    if not source_hash and source.get('mp3_path'):
                        mp3_path = source['mp3_path']
                        if os.path.exists(mp3_path):
                            source_hash = self.registry.get_file_hash(mp3_path)

                    if source_hash and source_text:
                        self.registry.register_source_transcription(
                            source_hash=source_hash,
                            transcription_text=source_text,
                            duration_seconds=source_duration,
                            original_file=original_file
                        )

                    current_pos = end_pos

            # Afficher les stats de d√©duplication
            dedup_stats = self.registry.get_source_deduplication_stats()
            logger.info(f"üìä [DEDUP STATS] {dedup_stats['total_cached_transcriptions']} sources cach√©es, "
                       f"{dedup_stats['total_cache_hits']} r√©utilisations")

        except Exception as e:
            logger.error(f"‚ùå [SOURCE CACHE] Erreur: {e}")
            import traceback
            logger.debug(traceback.format_exc())

    # ============================================================
    # M√âTHODES DE D√âDUPLICATION AVANC√âE - PHASE PR√â-TRANSCRIPTION
    # ============================================================

    def _build_source_hash_index(self, super_files: List[str]) -> Dict[str, List[Tuple[str, Dict]]]:
        """
        Construit un index global de tous les hash sources ‚Üí SuperFiles qui les contiennent.

        Args:
            super_files: Liste des chemins de SuperFiles

        Returns:
            Dict {source_hash: [(superfile_path, source_detail), ...]}
        """
        logger.info("\n" + "="*60)
        logger.info("[DEDUP] PHASE 1: Construction de l'index des sources")
        logger.info("="*60)

        hash_index = {}  # hash -> [(superfile, source_detail), ...]
        total_sources = 0

        for sf_path in super_files:
            source_info = self._get_superfile_source_info(sf_path)

            if not source_info:
                continue

            for detail in source_info:
                # Obtenir ou calculer le hash
                source_hash = detail.get('hash')
                if not source_hash and detail.get('mp3_path'):
                    mp3_path = detail['mp3_path']
                    if os.path.exists(mp3_path):
                        source_hash = self.registry.get_file_hash(mp3_path)
                        detail['hash'] = source_hash

                if source_hash:
                    if source_hash not in hash_index:
                        hash_index[source_hash] = []
                    hash_index[source_hash].append((sf_path, detail))
                    total_sources += 1

        # Statistiques
        unique_hashes = len(hash_index)
        duplicated_hashes = sum(1 for h, locs in hash_index.items() if len(locs) > 1)

        logger.info(f"   Total sources index√©es: {total_sources}")
        logger.info(f"   Hash uniques: {unique_hashes}")
        logger.info(f"   Hash dupliqu√©s (>1 SuperFile): {duplicated_hashes}")
        logger.info(f"   Taux de duplication: {(1 - unique_hashes/total_sources)*100:.1f}%" if total_sources > 0 else "N/A")

        return hash_index

    def _classify_superfiles(self, super_files: List[str], hash_index: Dict) -> Tuple[List[str], List[str]]:
        """
        Classifie les SuperFiles en deux cat√©gories:
        - to_transcribe: SuperFiles qui ont au moins une source non cach√©e
        - to_assemble: SuperFiles dont TOUTES les sources sont d√©j√† cach√©es

        Args:
            super_files: Liste des chemins de SuperFiles
            hash_index: Index hash ‚Üí SuperFiles

        Returns:
            Tuple (to_transcribe, to_assemble)
        """
        logger.info("\n" + "="*60)
        logger.info("[DEDUP] PHASE 2: Classification des SuperFiles")
        logger.info("="*60)

        # R√©cup√©rer tous les hash d√©j√† en cache
        cached_hashes = self.registry.get_all_cached_source_hashes()
        logger.info(f"   Hash d√©j√† en cache: {len(cached_hashes)}")

        to_transcribe = []
        to_assemble = []

        for sf_path in super_files:
            source_info = self._get_superfile_source_info(sf_path)

            if not source_info:
                # Pas d'info source, transcrire par s√©curit√©
                to_transcribe.append(sf_path)
                continue

            # Collecter les hash de ce SuperFile
            sf_hashes = set()
            for detail in source_info:
                source_hash = detail.get('hash')
                if not source_hash and detail.get('mp3_path'):
                    mp3_path = detail['mp3_path']
                    if os.path.exists(mp3_path):
                        source_hash = self.registry.get_file_hash(mp3_path)
                if source_hash:
                    sf_hashes.add(source_hash)

            # V√©rifier si TOUTES les sources sont en cache
            if sf_hashes and sf_hashes.issubset(cached_hashes):
                to_assemble.append(sf_path)
                logger.debug(f"   [ASSEMBLE] {os.path.basename(sf_path)} - {len(sf_hashes)} sources en cache")
            else:
                to_transcribe.append(sf_path)
                uncached = sf_hashes - cached_hashes
                logger.debug(f"   [TRANSCRIBE] {os.path.basename(sf_path)} - {len(uncached)} sources nouvelles")

        logger.info(f"\n   R√©sultat classification:")
        logger.info(f"   √Ä transcrire (API): {len(to_transcribe)}")
        logger.info(f"   √Ä assembler (cache): {len(to_assemble)}")
        if super_files:
            savings_pct = len(to_assemble) / len(super_files) * 100
            logger.info(f"   √âconomie potentielle: {savings_pct:.1f}%")

        return to_transcribe, to_assemble

    def _assemble_from_cache(self, sf_path: str) -> Optional[str]:
        """
        Assemble une transcription compl√®te depuis le cache des sources.

        Args:
            sf_path: Chemin du SuperFile

        Returns:
            Transcription assembl√©e ou None si √©chec
        """
        try:
            source_info = self._get_superfile_source_info(sf_path)

            if not source_info:
                logger.warning(f"[DEDUP] Pas d'info source pour assemblage: {os.path.basename(sf_path)}")
                return None

            # Collecter les hash dans l'ordre
            source_hashes = []
            for detail in source_info:
                source_hash = detail.get('hash')
                if not source_hash and detail.get('mp3_path'):
                    mp3_path = detail['mp3_path']
                    if os.path.exists(mp3_path):
                        source_hash = self.registry.get_file_hash(mp3_path)
                if source_hash:
                    source_hashes.append(source_hash)

            if not source_hashes:
                logger.warning(f"[DEDUP] Aucun hash source pour: {os.path.basename(sf_path)}")
                return None

            # R√©cup√©rer les transcriptions en batch
            cached_transcriptions = self.registry.batch_get_source_transcriptions(source_hashes)

            if len(cached_transcriptions) != len(source_hashes):
                missing = len(source_hashes) - len(cached_transcriptions)
                logger.warning(f"[DEDUP] {missing} sources manquantes en cache pour: {os.path.basename(sf_path)}")
                return None

            # Assembler dans l'ordre original
            assembled_parts = []
            for source_hash in source_hashes:
                if source_hash in cached_transcriptions:
                    text = cached_transcriptions[source_hash].get('text', '')
                    if text:
                        assembled_parts.append(text)
                    self.stats['source_cache_hits'] += 1

            if not assembled_parts:
                logger.warning(f"[DEDUP] Assemblage vide pour: {os.path.basename(sf_path)}")
                return None

            # Joindre les parties avec espacement
            assembled_text = "\n\n".join(assembled_parts)

            logger.info(f"[DEDUP] Assembl√© depuis cache: {os.path.basename(sf_path)} "
                       f"({len(assembled_parts)} sources, {len(assembled_text)} chars)")

            return assembled_text

        except Exception as e:
            logger.error(f"[DEDUP] Erreur assemblage {os.path.basename(sf_path)}: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return None

    def _save_assembled_transcription(self, sf_path: str, transcription: str) -> bool:
        """
        Sauvegarde une transcription assembl√©e comme si elle avait √©t√© transcrite normalement.

        Args:
            sf_path: Chemin du SuperFile
            transcription: Texte assembl√©

        Returns:
            True si succ√®s
        """
        try:
            # Calculer le hash du SuperFile
            file_hash = self.registry.get_file_hash(sf_path)

            if not file_hash:
                logger.error(f"[DEDUP] Impossible de calculer hash pour: {sf_path}")
                return False

            # Enregistrer dans le registry
            self.registry.register_transcription(file_hash, transcription, sf_path)

            # Ajouter au cache local
            self.cache.put(file_hash, transcription)

            # Sauvegarder dans un fichier .txt
            txt_file = sf_path + '.txt'
            with open(txt_file, 'w', encoding='utf-8') as f:
                f.write(transcription)

            logger.debug(f"[DEDUP] Sauvegard√©: {os.path.basename(txt_file)}")

            # ‚úÖ NOUVEAU : Cr√©er aussi un segments.json minimal avec source_files_details
            segments_file = sf_path + '.segments.json'
            source_details = self._get_superfile_source_info(sf_path)

            segments_data = {
                'file': os.path.basename(sf_path),
                'total_duration': 0,  # Non calcul√© pour assembl√©
                'segments_count': 0,  # Pas de segments Whisper (assembl√©)
                'segments': [],  # Vide car assembl√© depuis chunks
                'assembled_from_cache': True  # Flag pour identifier source
            }

            if source_details:
                segments_data['source_files_details'] = source_details
                # Calculer total_duration depuis source_details
                total_dur = sum(s.get('duration', 0) for s in source_details)
                segments_data['total_duration'] = total_dur
                logger.debug(f"‚úÖ [ASSEMBLED] segments.json cr√©√© avec {len(source_details)} sources")
            else:
                logger.warning(f"‚ö†Ô∏è [ASSEMBLED] Aucun source_details pour {os.path.basename(sf_path)}")

            with open(segments_file, 'w', encoding='utf-8') as f:
                import json
                json.dump(segments_data, f, ensure_ascii=False, indent=2)

            self.stats['files_assembled_from_cache'] += 1
            return True

        except Exception as e:
            logger.error(f"[DEDUP] Erreur sauvegarde assemblage: {e}")
            return False

    def _is_incremental_superfile(self, file_path: str) -> bool:
        """D√©termine si un SuperFile a √©t√© cr√©√© incr√©mentalement (nouveaux audios d√©tect√©s)"""
        try:
            # V√©rifier si le fichier a √©t√© cr√©√© r√©cemment (moins de 5 minutes)
            file_mtime = os.path.getmtime(file_path)
            current_time = time.time()
            age_minutes = (current_time - file_mtime) / 60
            
            if age_minutes < 5:
                logger.debug(f"[INCR√âMENTAL] {os.path.basename(file_path)} cr√©√© il y a {age_minutes:.1f} minutes")
                return True
                
            # V√©rifier si le hash du fichier n'existe pas encore dans le registry
            file_hash = self.registry.get_file_hash(file_path)
            existing_transcription = self.registry.get_transcription(file_hash)
            
            if not existing_transcription:
                logger.debug(f"[INCR√âMENTAL] {os.path.basename(file_path)} pas encore transcrit")
                return True
                
            return False
        except Exception as e:
            logger.debug(f"[INCR√âMENTAL] Erreur v√©rification {file_path}: {e}")
            return False

    def _optimize_processing_order(self, files: List[str]) -> List[str]:
        """Optimise l'ordre: petits fichiers d'abord pour saturation rapide"""
        file_sizes = []
        for file in files:
            try:
                size = os.path.getsize(file)
                file_sizes.append((file, size))
            except:
                file_sizes.append((file, 0))

        # Trier par taille croissante
        file_sizes.sort(key=lambda x: x[1])
        return [f[0] for f in file_sizes]

    def _prepare_work_items(self, files: List[str]) -> List[Dict]:
        """Pr√©pare les √©l√©ments de travail avec chunking si n√©cessaire"""
        work_items = []

        for file_path in files:
            if self.chunker.needs_chunking(file_path):
                # Fichier trop gros, chunking n√©cessaire
                size_mb = os.path.getsize(file_path) / (1024 * 1024)
                logger.info(f"üî™ Chunking requis pour {os.path.basename(file_path)} ({size_mb:.1f}MB)")

                chunks = self.chunker.chunk_audio(file_path)
                self.stats['chunks_created'] += len(chunks)

                for i, chunk_path in enumerate(chunks):
                    work_items.append({
                        'original_file': file_path,
                        'chunk_file': chunk_path,
                        'chunk_index': i,
                        'total_chunks': len(chunks),
                        'is_chunk': True
                    })
            else:
                # Fichier OK, pas de chunking
                work_items.append({
                    'original_file': file_path,
                    'chunk_file': file_path,
                    'chunk_index': 0,
                    'total_chunks': 1,
                    'is_chunk': False
                })

        return work_items

    def _execute_parallel_transcription(self, work_items: List[Dict], files_to_assemble: List[str] = None) -> List[Dict]:
        """
        Ex√©cute les transcriptions en 2 phases:
        - Phase 1: Transcription API des fichiers avec sources nouvelles
        - Phase 2: Assemblage depuis cache des fichiers dont toutes les sources sont cach√©es
        """
        results = {}  # Group√© par fichier original
        files_to_assemble = files_to_assemble or []

        # ============================================================
        # PHASE 1: Transcription API (fichiers avec sources nouvelles)
        # ============================================================
        if work_items:
            logger.info("\n" + "="*60)
            logger.info("üöÄ PHASE 1: TRANSCRIPTION API")
            logger.info(f"   {len(work_items)} √©l√©ments √† transcrire")
            logger.info("="*60)

            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # Soumettre tous les travaux
                futures = []
                for item in work_items:
                    future = executor.submit(self._transcribe_single_item, item)
                    futures.append((future, item))

                # Collecter les r√©sultats
                completed = 0
                total = len(futures)

                for future, item in futures:
                    try:
                        result = future.result(timeout=600)  # 10 minutes max par fichier

                        # Grouper les r√©sultats par fichier original
                        original_file = item['original_file']
                        if original_file not in results:
                            results[original_file] = {
                                'chunks': {},
                                'total_chunks': item['total_chunks'],
                                'success': True
                            }

                        if result['success']:
                            if item['is_chunk']:
                                results[original_file]['chunks'][item['chunk_index']] = result['text']
                            else:
                                results[original_file]['text'] = result['text']
                                # Transf√©rer le flag 'saved' pour √©viter double sauvegarde
                                if result.get('saved'):
                                    results[original_file]['saved'] = True
                        else:
                            results[original_file]['success'] = False
                            self.stats['errors'] += 1

                        completed += 1
                        progress = (completed / total) * 100

                        # Calcul des m√©triques en temps r√©el
                        elapsed = time.time() - self.stats.get('start_time', time.time())
                        rate = completed / elapsed if elapsed > 0 else 0
                        eta = (total - completed) / rate if rate > 0 else 0

                        # Log √† chaque fichier trait√©
                        logger.info(f"üìä [{completed}/{total}] {progress:.0f}% | ‚ö° {rate:.1f} fichiers/min | ‚è±Ô∏è ETA: {eta/60:.1f} min")

                        # Stats d√©taill√©es tous les 3 fichiers
                        if completed % 3 == 0 or completed == total:
                            active_keys = len([k for k,v in self.api_pool.health_scores.items() if v > 0])
                            logger.info(f"   üíæ Cache: {self.cache.hits} hits / {self.cache.misses} miss")
                            logger.info(f"   üîë Cl√©s actives: {active_keys}/{len(self.api_pool.api_keys)}")
                            logger.info(f"   ‚úÖ Succ√®s: {completed - self.stats.get('errors', 0)}/{completed}")

                    except Exception as e:
                        import traceback
                        logger.error(f"‚ùå Erreur collecte r√©sultat: {type(e).__name__}: {e}")
                        logger.error(f"   Fichier: {item.get('original_file', 'inconnu')}")
                        logger.error(f"   Traceback: {traceback.format_exc()}")
                        self.stats['errors'] += 1

            # Consolider les r√©sultats et sauvegarder
            self._consolidate_and_save_results(results)

        # ============================================================
        # PHASE 2: Assemblage depuis cache (sources d√©j√† transcrites)
        # ============================================================
        if files_to_assemble:
            logger.info("\n" + "="*60)
            logger.info("üîÑ PHASE 2: ASSEMBLAGE DEPUIS CACHE")
            logger.info(f"   {len(files_to_assemble)} fichiers √† assembler")
            logger.info("="*60)

            assembled_count = 0
            for sf_path in files_to_assemble:
                file_name = os.path.basename(sf_path)
                logger.info(f"\nüìÑ Assemblage: {file_name}")

                # Assembler depuis le cache
                assembled_text = self._assemble_from_cache(sf_path)

                if assembled_text:
                    # Sauvegarder comme une transcription normale
                    success = self._save_assembled_transcription(sf_path, assembled_text)
                    if success:
                        assembled_count += 1
                        logger.info(f"   ‚úÖ Assembl√© et sauvegard√©: {len(assembled_text)} caract√®res")
                    else:
                        logger.error(f"   ‚ùå Erreur sauvegarde assemblage")
                        self.stats['errors'] += 1
                else:
                    # √âchec d'assemblage - ajouter √† la file de transcription?
                    # Pour l'instant on log l'erreur
                    logger.warning(f"   ‚ö†Ô∏è Impossible d'assembler {file_name} - sources manquantes en cache")

            logger.info(f"\nüìä Phase 2 termin√©e: {assembled_count}/{len(files_to_assemble)} fichiers assembl√©s")

        return list(results.values())

    def _transcribe_single_item(self, work_item: Dict) -> Dict:
        """Transcrit un seul √©l√©ment (fichier ou chunk)"""
        file_path = work_item['chunk_file']
        file_name = os.path.basename(file_path)
        compressed_file = None  # Track pour cleanup

        logger.info(f"\n" + "="*60)
        logger.info(f"üéØ D√âBUT TRANSCRIPTION: {file_name}")
        logger.info("="*60)

        # üîß OPTIMISATION: Compresser le fichier avant envoi √† l'API
        original_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
        file_to_send = file_path

        if original_size > 5:  # Compresser si > 5MB
            compressed_path, compression_success = compress_audio_for_whisper(file_path)
            if compression_success and compressed_path != file_path:
                file_to_send = compressed_path
                compressed_file = compressed_path  # Marquer pour cleanup
                logger.info(f"üì¶ Utilisation du fichier compress√© pour transcription")
            else:
                logger.info(f"üì¶ Utilisation du fichier original (compression non n√©cessaire/√©chec)")

        # Obtenir un client API disponible
        logger.info("üîç Recherche d'une cl√© API disponible...")
        api_key, client = self.api_pool.get_best_client()
        if not client:
            # Attendre qu'une cl√© se lib√®re
            time.sleep(10)
            api_key, client = self.api_pool.get_best_client()
            if not client:
                # Cleanup fichier compress√© si √©chec
                if compressed_file and os.path.exists(compressed_file):
                    os.remove(compressed_file)
                return {
                    'success': False,
                    'error': 'Aucune cl√© API disponible'
                }

        # Tentatives de transcription avec retry
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # V√©rifier la taille du fichier √† envoyer
                file_size = os.path.getsize(file_to_send)
                if file_size > 25 * 1024 * 1024:  # 25MB
                    logger.error(f"Fichier trop gros malgr√© compression: {file_size / (1024*1024):.1f}MB")
                    if compressed_file and os.path.exists(compressed_file):
                        os.remove(compressed_file)
                    return {'success': False, 'error': 'Fichier trop gros'}

                # Appel API OpenAI Whisper
                logger.info(f"\nüéôÔ∏è [OPENAI WHISPER API]")
                logger.info(f"   üìÅ Fichier: {os.path.basename(file_to_send)} ({file_size / (1024*1024):.1f}MB)")
                logger.info(f"   üîë Cl√© API: {api_key[:20]}...{api_key[-4:]}")
                logger.info(f"   üïë Heure: {datetime.now().strftime('%H:%M:%S')}")
                logger.info(f"   üì° ENVOI EN COURS √Ä OPENAI...")

                api_start = time.time()
                with open(file_to_send, 'rb') as audio_file:
                    # üÜï verbose_json pour avoir les timestamps des segments
                    api_response = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=audio_file,
                        response_format="verbose_json",  # üÜï Pour avoir les segments avec timestamps
                        language="fr"
                    )
                api_duration = time.time() - api_start

                logger.info(f"   ‚úÖ R√âPONSE RE√áUE en {api_duration:.1f} secondes!")

                # Succ√®s!
                self.api_pool.update_client_health(api_key, True)
                self.stats['api_calls'] += 1

                # üÜï Extraire le texte et les segments de la r√©ponse verbose_json
                if hasattr(api_response, 'text'):
                    response = api_response.text
                    segments = []
                    if hasattr(api_response, 'segments') and api_response.segments:
                        # Debug: voir le type du premier segment
                        first_seg = api_response.segments[0]
                        logger.info(f"   üîç Type segment: {type(first_seg).__name__}")
                        logger.info(f"   üîç Attributs: {dir(first_seg)[:10]}...")

                        for seg in api_response.segments:
                            # G√©rer les segments comme dictionnaire OU objet Pydantic
                            if isinstance(seg, dict):
                                seg_start = seg.get('start', 0)
                                seg_end = seg.get('end', 0)
                                seg_text = seg.get('text', '')
                            elif hasattr(seg, 'model_dump'):
                                # Pydantic v2 model
                                seg_dict = seg.model_dump()
                                seg_start = seg_dict.get('start', 0)
                                seg_end = seg_dict.get('end', 0)
                                seg_text = seg_dict.get('text', '')
                            elif hasattr(seg, 'dict'):
                                # Pydantic v1 model
                                seg_dict = seg.dict()
                                seg_start = seg_dict.get('start', 0)
                                seg_end = seg_dict.get('end', 0)
                                seg_text = seg_dict.get('text', '')
                            else:
                                # Acc√®s direct aux attributs
                                seg_start = getattr(seg, 'start', 0)
                                seg_end = getattr(seg, 'end', 0)
                                seg_text = getattr(seg, 'text', '')

                            segments.append({
                                'start': seg_start,
                                'end': seg_end,
                                'text': seg_text
                            })
                        logger.info(f"   üìä {len(segments)} segments avec timestamps extraits")
                        # Debug: afficher le premier segment pour v√©rifier
                        if segments:
                            logger.info(f"   üîç Premier segment extrait: start={segments[0].get('start')}, end={segments[0].get('end')}")
                            if segments[0].get('end', 0) > 0:
                                logger.info(f"   ‚úÖ Timestamps OK: {segments[0]['start']:.1f}s - {segments[0]['end']:.1f}s")
                else:
                    # Fallback si format inattendu
                    response = str(api_response)
                    segments = []

                # Afficher un aper√ßu de la transcription
                text_length = len(response)
                preview = response[:200].replace('\n', ' ')
                if text_length > 200:
                    preview += "..."

                logger.info(f"\n‚úÖ TRANSCRIPTION R√âUSSIE!")
                logger.info(f"   üìù Longueur: {text_length} caract√®res")
                logger.info(f"   üîç Aper√ßu: \"{preview}\"")
                logger.info(f"{'='*60}\n")

                # üîß FIX BUG #1: Sauvegarder IMM√âDIATEMENT apr√®s succ√®s API
                # Ne pas attendre la consolidation qui peut perdre des transcriptions
                # ‚ö†Ô∏è SAUF pour les chunks - ils doivent √™tre assembl√©s d'abord
                is_chunk = work_item.get('is_chunk', False)
                saved_immediately = False

                if not is_chunk:
                    # Fichier complet (non-chunk√©) - sauvegarder maintenant
                    original_file = work_item.get('original_file', file_path)
                    try:
                        file_hash = self.registry.get_file_hash(original_file)
                        if file_hash:
                            # Sauvegarder dans le Registry MAINTENANT
                            self.registry.register_transcription(file_hash, response, original_file)
                            logger.info(f"üíæ [SAUVEGARDE IMM√âDIATE] Transcription enregistr√©e pour {os.path.basename(original_file)}")

                            # Sauvegarder aussi dans un fichier .txt
                            txt_file = original_file + '.txt'
                            with open(txt_file, 'w', encoding='utf-8') as f:
                                f.write(response if isinstance(response, str) else str(response))
                            logger.info(f"üìÑ Fichier .txt cr√©√©: {os.path.basename(txt_file)}")

                            # üÜï Sauvegarder les segments avec timestamps dans un fichier JSON
                            if segments:
                                segments_file = original_file + '.segments.json'

                                # ‚úÖ NOUVEAU : R√©cup√©rer source_files_details depuis registry
                                source_details = self._get_superfile_source_info(original_file)

                                segments_data = {
                                    'file': os.path.basename(original_file),
                                    'total_duration': segments[-1]['end'] if segments else 0,
                                    'segments_count': len(segments),
                                    'segments': segments
                                }

                                # ‚úÖ NOUVEAU : Injecter source_files_details si disponible
                                if source_details:
                                    segments_data['source_files_details'] = source_details
                                    logger.debug(f"‚úÖ [SOURCE FILES] {len(source_details)} fichiers sources ajout√©s au segments.json")
                                else:
                                    logger.warning(f"‚ö†Ô∏è [SOURCE FILES] Aucun d√©tail de source trouv√© pour {os.path.basename(original_file)}")

                                with open(segments_file, 'w', encoding='utf-8') as f:
                                    import json
                                    json.dump(segments_data, f, ensure_ascii=False, indent=2)

                                logger.info(f"üìä Fichier segments cr√©√©: {os.path.basename(segments_file)} ({len(segments)} segments)")

                            # Ajouter au cache
                            self.cache.put(file_hash, response)
                            saved_immediately = True

                            # üÜï D√âDUPLICATION: Enregistrer par hash des fichiers sources
                            # Cela permet de r√©utiliser les transcriptions pour les fichiers dupliqu√©s
                            self._cache_transcription_by_source_files(original_file, response, segments)

                    except Exception as save_error:
                        logger.error(f"‚ùå Erreur sauvegarde imm√©diate: {save_error}")
                        import traceback
                        logger.error(traceback.format_exc())
                else:
                    logger.debug(f"‚è≠Ô∏è Chunk {work_item.get('chunk_index', '?')} - sauvegarde diff√©r√©e pour assemblage")

                # üßπ Cleanup: Supprimer le fichier compress√© apr√®s succ√®s
                if compressed_file and os.path.exists(compressed_file):
                    try:
                        os.remove(compressed_file)
                        logger.debug(f"üßπ Fichier compress√© supprim√©: {os.path.basename(compressed_file)}")
                    except Exception as cleanup_error:
                        logger.warning(f"‚ö†Ô∏è Impossible de supprimer fichier compress√©: {cleanup_error}")

                return {
                    'success': True,
                    'text': response,
                    'segments': segments,  # üÜï Segments avec timestamps pour segmentation
                    'saved': saved_immediately  # True seulement si non-chunk ET sauvegard√©
                }

            except Exception as e:
                error_str = str(e).lower()

                logger.error(f"\n‚ùå ERREUR lors de la transcription")
                logger.error(f"   Tentative: {attempt+1}/{max_retries}")
                logger.error(f"   Type: {type(e).__name__}")
                logger.error(f"   Message: {str(e)}")

                # Gestion des erreurs sp√©cifiques
                if 'rate' in error_str or '429' in error_str:
                    logger.warning(f"   üïë RATE LIMIT! Rotation de cl√©...")
                    self.api_pool.update_client_health(api_key, False, 'rate_limit')
                    time.sleep(5)  # Attendre 5 secondes

                    # Obtenir une autre cl√©
                    api_key, client = self.api_pool.get_best_client()
                    if not client:
                        time.sleep(30)  # Attendre plus longtemps
                        api_key, client = self.api_pool.get_best_client()

                elif 'quota' in error_str:
                    logger.error(f"   üí≥ QUOTA D√âPASS√â pour la cl√©!")
                    self.api_pool.update_client_health(api_key, False, 'quota_exceeded')
                    # Cleanup fichier compress√©
                    if compressed_file and os.path.exists(compressed_file):
                        os.remove(compressed_file)
                    return {'success': False, 'error': 'Quota d√©pass√©'}

                else:
                    self.api_pool.update_client_health(api_key, False, 'other')

                if attempt == max_retries - 1:
                    logger.error(f"   ‚ùå √âCHEC D√âFINITIF apr√®s {max_retries} tentatives")

                    # üÜï ENREGISTRER L'√âCHEC pour √©viter boucle infinie
                    # MAIS seulement pour les fichiers complets, pas les chunks
                    is_chunk = work_item.get('is_chunk', False)

                    if not is_chunk:
                        try:
                            # R√©cup√©rer le fichier original et calculer son hash
                            original_file = work_item.get('original_file', file_path)
                            file_hash = self.registry.get_file_hash(original_file)

                            if file_hash:
                                # Utiliser la m√©thode EXISTANTE du registry
                                self.registry.register_failed_transcription(
                                    file_hash=file_hash,
                                    error=f"{type(e).__name__}: {str(e)}"
                                )
                                # Log s√©curis√© - v√©rifier que l'entr√©e existe
                                attempts_count = self.registry.data.get('failed_transcriptions', {}).get(file_hash, {}).get('attempts', '?')
                                logger.warning(f"‚ö†Ô∏è  √âchec enregistr√© pour {os.path.basename(original_file)} - tentative #{attempts_count}/3")
                            else:
                                logger.warning(f"‚ö†Ô∏è  Hash impossible pour {os.path.basename(original_file)} - √©chec non enregistr√©")

                        except Exception as record_error:
                            logger.error(f"‚ùå Erreur enregistrement √©chec: {record_error}")
                            import traceback
                            logger.debug(traceback.format_exc())
                    else:
                        logger.debug(f"‚è≠Ô∏è  Chunk √©chec - pas d'enregistrement (sera regroup√© avec fichier parent)")

                    # Cleanup fichier compress√©
                    if compressed_file and os.path.exists(compressed_file):
                        os.remove(compressed_file)
                    return {'success': False, 'error': str(e)}

                # Backoff exponentiel
                wait_time = 2 ** attempt
                logger.info(f"   ‚è≥ Attente de {wait_time} secondes avant retry...")
                time.sleep(wait_time)

        # Cleanup fichier compress√© si √©chec final
        if compressed_file and os.path.exists(compressed_file):
            os.remove(compressed_file)
        return {'success': False, 'error': 'Max retries'}

    def _consolidate_and_save_results(self, results: Dict):
        """Consolide les chunks et sauvegarde les transcriptions"""
        for file_path, result in results.items():
            if not result.get('success'):
                continue

            # üîß FIX: Skip si d√©j√† sauvegard√© imm√©diatement
            if result.get('saved'):
                logger.debug(f"‚è≠Ô∏è Skip consolidation pour {os.path.basename(file_path)} (d√©j√† sauvegard√©)")
                self.stats['files_processed'] += 1
                continue

            # Reconstituer le texte complet pour les chunks
            if 'chunks' in result and result['chunks']:
                # Assembler les chunks dans l'ordre
                full_text = ""
                for i in range(result['total_chunks']):
                    if i in result['chunks']:
                        full_text += result['chunks'][i] + " "
                result['text'] = full_text.strip()

            if 'text' in result and result['text']:
                try:
                    # Calculer le hash du fichier
                    file_hash = self.registry.get_file_hash(file_path)

                    # Enregistrer dans le Registry
                    self.registry.register_transcription(file_hash, result['text'], file_path)

                    # Ajouter au cache
                    self.cache.put(file_hash, result['text'])

                    # Sauvegarder dans un fichier .txt
                    self._save_transcription_file(file_path, result['text'])

                    self.stats['files_processed'] += 1
                    logger.info(f"‚úÖ Consolidation r√©ussie pour {os.path.basename(file_path)}")
                except Exception as e:
                    logger.error(f"‚ùå Erreur consolidation pour {os.path.basename(file_path)}: {e}")
                    import traceback
                    logger.error(traceback.format_exc())

    def _save_transcription_file(self, audio_file: str, transcription: str):
        """Sauvegarde la transcription dans un fichier .txt"""
        try:
            txt_file = audio_file + '.txt'
            with open(txt_file, 'w', encoding='utf-8') as f:
                f.write(transcription)
            logger.debug(f"Transcription sauvegard√©e: {os.path.basename(txt_file)}")
        except Exception as e:
            logger.error(f"Erreur sauvegarde transcription: {e}")

    def _print_statistics(self):
        """Affiche les statistiques de performance"""
        duration = time.time() - self.stats['start_time']

        logger.info("="*60)
        logger.info("üìä STATISTIQUES TRANSCRIBER ULTRA")
        logger.info("="*60)
        logger.info(f"‚è±Ô∏è  Dur√©e: {duration:.1f} secondes")
        logger.info(f"‚úÖ Transcrits: {self.stats['files_processed']}")
        logger.info(f"üíæ Depuis cache: {self.stats['files_cached']}")
        logger.info(f"‚è≠Ô∏è  D√©j√† trait√©s: {self.stats['files_skipped']}")
        logger.info(f"üî™ Chunks cr√©√©s: {self.stats['chunks_created']}")
        logger.info(f"üì° Appels API: {self.stats['api_calls']}")
        logger.info(f"‚ùå Erreurs: {self.stats['errors']}")

        if duration > 0:
            files_per_minute = (self.stats['files_processed'] / duration) * 60
            logger.info(f"‚ö° Performance: {files_per_minute:.1f} fichiers/minute")

        # Stats du pool API
        api_stats = self.api_pool.get_stats()
        logger.info(f"üîë Cl√©s actives: {api_stats['active_keys']}/{api_stats['total_keys']}")
        if api_stats['total_requests'] > 0:
            logger.info(f"‚úÖ Taux succ√®s: {api_stats['success_rate']:.1f}%")

        # Stats du cache
        cache_stats = self.cache.get_stats()
        logger.info(f"üíæ Cache: {cache_stats['entries']} entr√©es, {cache_stats['hit_rate']:.1f}% hit rate")

        logger.info("="*60)


# Point d'entr√©e pour tests
if __name__ == "__main__":
    logger.info("TranscriberUltra module charg√© avec succ√®s!")
    logger.info("Pour utiliser: activer 'ultra_mode = True' dans config.ini")